{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OFqbnI616-0M",
        "outputId": "f34644ad-3069-419e-8233-4dbf07deae10"
      },
      "id": "OFqbnI616-0M",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "PATH = r\"/content/gdrive/My Drive/Colab Notebooks/gec24\"\n",
        "os.chdir(PATH)"
      ],
      "metadata": {
        "id": "nyMk7ND169Kx"
      },
      "id": "nyMk7ND169Kx",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "initial_id",
      "metadata": {
        "collapsed": true,
        "id": "initial_id"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import sentencepiece as spm\n",
        "import torch.nn as nn\n",
        "import random"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Loading the architecture for the model as only the parameters were saved with save_dict"
      ],
      "metadata": {
        "id": "6tEEI6sFQ9Od"
      },
      "id": "6tEEI6sFQ9Od"
    },
    {
      "metadata": {
        "id": "e857d8523f38750a"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null,
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, input_dim, embedding_dim, hidden_dim, n_layers, dropout):\n",
        "        super().__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.n_layers = n_layers\n",
        "        self.embedding = nn.Embedding(input_dim, embedding_dim)\n",
        "        self.rnn = nn.LSTM(embedding_dim, hidden_dim, n_layers, dropout=dropout)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, src):\n",
        "        # src = [src length, batch size]\n",
        "        embedded = self.dropout(self.embedding(src))\n",
        "        # embedded = [src length, batch size, embedding dim]\n",
        "        outputs, (hidden, cell) = self.rnn(embedded)\n",
        "        # outputs = [src length, batch size, hidden dim * n directions]\n",
        "        # hidden = [n layers * n directions, batch size, hidden dim]\n",
        "        # cell = [n layers * n directions, batch size, hidden dim]\n",
        "        # outputs are always from the top hidden layer\n",
        "        return hidden, cell"
      ],
      "id": "e857d8523f38750a"
    },
    {
      "metadata": {
        "id": "ac75933384a66b55"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null,
      "source": [
        "class Decoder(nn.Module):\n",
        "    def __init__(self, output_dim, embedding_dim, hidden_dim, n_layers, dropout):\n",
        "        super().__init__()\n",
        "        self.output_dim = output_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.n_layers = n_layers\n",
        "        self.embedding = nn.Embedding(output_dim, embedding_dim)\n",
        "        self.rnn = nn.LSTM(embedding_dim, hidden_dim, n_layers, dropout=dropout)\n",
        "        self.fc_out = nn.Linear(hidden_dim, output_dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, input, hidden, cell):\n",
        "        # input = [batch size]\n",
        "        # hidden = [n layers * n directions, batch size, hidden dim]\n",
        "        # cell = [n layers * n directions, batch size, hidden dim]\n",
        "        # n directions in the decoder will both always be 1, therefore:\n",
        "        # hidden = [n layers, batch size, hidden dim]\n",
        "        # context = [n layers, batch size, hidden dim]\n",
        "        input = input.unsqueeze(0)\n",
        "        # input = [1, batch size]\n",
        "        embedded = self.dropout(self.embedding(input))\n",
        "        # embedded = [1, batch size, embedding dim]\n",
        "        output, (hidden, cell) = self.rnn(embedded, (hidden, cell))\n",
        "        # output = [seq length, batch size, hidden dim * n directions]\n",
        "        # hidden = [n layers * n directions, batch size, hidden dim]\n",
        "        # cell = [n layers * n directions, batch size, hidden dim]\n",
        "        # seq length and n directions will always be 1 in this decoder, therefore:\n",
        "        # output = [1, batch size, hidden dim]\n",
        "        # hidden = [n layers, batch size, hidden dim]\n",
        "        # cell = [n layers, batch size, hidden dim]\n",
        "        prediction = self.fc_out(output.squeeze(0))\n",
        "        # prediction = [batch size, output dim]\n",
        "        return prediction, hidden, cell"
      ],
      "id": "ac75933384a66b55"
    },
    {
      "metadata": {
        "id": "633cbcc7bb81aa33"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null,
      "source": [
        "class Seq2Seq(nn.Module):\n",
        "    def __init__(self, encoder, decoder, device):\n",
        "        super().__init__()\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.device = device\n",
        "        self.to(device)\n",
        "        assert (\n",
        "            encoder.hidden_dim == decoder.hidden_dim\n",
        "        ), \"Hidden dimensions of encoder and decoder must be equal!\"\n",
        "        assert (\n",
        "            encoder.n_layers == decoder.n_layers\n",
        "        ), \"Encoder and decoder must have equal number of layers!\"\n",
        "\n",
        "    def forward(self, src, tgt, teacher_forcing_ratio):\n",
        "        # src = [src length, batch size]\n",
        "        # tgt = [tgt length, batch size]\n",
        "        # teacher_forcing_ratio is probability to use teacher forcing\n",
        "        # e.g. if teacher_forcing_ratio is 0.75 we use ground-truth inputs 75% of the time\n",
        "        batch_size = tgt.shape[1]\n",
        "        tgt_length = tgt.shape[0]\n",
        "        tgt_vocab_size = self.decoder.output_dim\n",
        "        # tensor to store decoder outputs\n",
        "        outputs = torch.zeros(tgt_length, batch_size, tgt_vocab_size).to(self.device)\n",
        "        # last hidden state of the encoder is used as the initial hidden state of the decoder\n",
        "        hidden, cell = self.encoder(src)\n",
        "        # hidden = [n layers * n directions, batch size, hidden dim]\n",
        "        # cell = [n layers * n directions, batch size, hidden dim]\n",
        "        # first input to the decoder is the <sos> tokens\n",
        "        input = tgt[0, :]\n",
        "        # input = [batch size]\n",
        "        for t in range(1, tgt_length):\n",
        "            # insert input token embedding, previous hidden and previous cell states\n",
        "            # receive output tensor (predictions) and new hidden and cell states\n",
        "            output, hidden, cell = self.decoder(input, hidden, cell)\n",
        "            # output = [batch size, output dim]\n",
        "            # hidden = [n layers, batch size, hidden dim]\n",
        "            # cell = [n layers, batch size, hidden dim]\n",
        "            # place predictions in a tensor holding predictions for each token\n",
        "            outputs[t] = output\n",
        "            # decide if we are going to use teacher forcing or not\n",
        "            teacher_force = random.random() < teacher_forcing_ratio\n",
        "            # get the highest predicted token from our predictions\n",
        "            top1 = output.argmax(1)\n",
        "            # if teacher forcing, use actual next token as next input\n",
        "            # if not, use predicted token\n",
        "            input = tgt[t] if teacher_force else top1\n",
        "            # input = [batch size]\n",
        "        return outputs"
      ],
      "id": "633cbcc7bb81aa33"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Defining a function on how to correct sentences"
      ],
      "metadata": {
        "id": "OSGTgW9MRnwV"
      },
      "id": "OSGTgW9MRnwV"
    },
    {
      "metadata": {
        "id": "488503406f699aa5"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null,
      "source": [
        "def correct_sentence(\n",
        "    sentence,\n",
        "    model,\n",
        "    sp,\n",
        "    sos_token,\n",
        "    eos_token,\n",
        "    device,\n",
        "    max_output_length=25,\n",
        "):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        # Tokenize sentence with SentencePiece\n",
        "        tokens = sp.encode_as_pieces(sentence)\n",
        "        tokens = [sos_token] + tokens + [eos_token]  # Add SOS and EOS tokens\n",
        "\n",
        "        # Encode tokens to get IDs\n",
        "        ids = sp.encode_as_ids(sentence)\n",
        "        tensor = torch.LongTensor(ids).unsqueeze(-1).to(device)\n",
        "\n",
        "        hidden, cell = model.encoder(tensor)\n",
        "\n",
        "        inputs = [sp.bos_id()]  # Use BOS token ID\n",
        "        for _ in range(max_output_length):\n",
        "            inputs_tensor = torch.LongTensor([inputs[-1]]).to(device)\n",
        "            output, hidden, cell = model.decoder(inputs_tensor, hidden, cell)\n",
        "            predicted_token_id = output.argmax(-1).item()\n",
        "            inputs.append(predicted_token_id)\n",
        "            if predicted_token_id == sp.eos_id():\n",
        "                break\n",
        "\n",
        "        # Decode predicted token IDs to tokens\n",
        "        predicted_tokens = sp.decode_ids(inputs)\n",
        "\n",
        "    return predicted_tokens"
      ],
      "id": "488503406f699aa5"
    },
    {
      "metadata": {
        "id": "32441e0b3c74f120"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null,
      "source": [
        "def load_test_targets(filename):\n",
        "    sentences = []\n",
        "    with open(filename, 'r') as file:\n",
        "        for line in file:\n",
        "            sentences.append(line.strip())\n",
        "    return sentences"
      ],
      "id": "32441e0b3c74f120"
    },
    {
      "metadata": {
        "id": "bc28f6f72a98d960"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null,
      "source": [
        "def generate_corrections(target_file, output_file, model, sp, sos_token, eos_token, device):\n",
        "    # Load original sentences\n",
        "    original_sentences = load_test_targets(target_file)\n",
        "\n",
        "    # correct and write to a file\n",
        "    with open(output_file, 'w+') as outfile:\n",
        "        for sentence in original_sentences:\n",
        "            corrected_sentence = correct_sentence(\n",
        "                sentence=sentence,\n",
        "                model=model,\n",
        "                sp=sp,\n",
        "                sos_token=sos_token,\n",
        "                eos_token=eos_token,\n",
        "                device=device\n",
        "            )\n",
        "            outfile.write(corrected_sentence + \"\\n\")\n",
        "\n",
        "    print(f\"Corrected sentences saved to {output_file}\")"
      ],
      "id": "bc28f6f72a98d960"
    },
    {
      "cell_type": "code",
      "source": [
        "model_parameters = {\n",
        "    \"model1\": {\n",
        "        \"tokenizer\": \"bpe_8000\",\n",
        "        \"batch_size\": 128,\n",
        "        \"embedding_dim\": 256,\n",
        "        \"hidden_dim\": 512,\n",
        "        \"n_layers\": 2,\n",
        "        \"dropout\": 0.5,\n",
        "        \"teacher_forcing_ratio\": 0.25,\n",
        "        \"clip\": 1.0,\n",
        "        \"epochs\": 20\n",
        "    },\n",
        "    \"model2\": {\n",
        "        \"tokenizer\": \"bpe_16000\",\n",
        "        \"batch_size\": 256,\n",
        "        \"embedding_dim\": 512,\n",
        "        \"hidden_dim\": 512,\n",
        "        \"n_layers\": 4,\n",
        "        \"dropout\": 0.5,\n",
        "        \"teacher_forcing_ratio\": 0.25,\n",
        "        \"clip\": 1.0,\n",
        "        \"epochs\": 30\n",
        "    },\n",
        "    \"model3\": {\n",
        "        \"tokenizer\": \"bpe_16000\",\n",
        "        \"batch_size\": 128,\n",
        "        \"embedding_dim\": 256,\n",
        "        \"hidden_dim\": 512,\n",
        "        \"n_layers\": 2,\n",
        "        \"dropout\": 0.5,\n",
        "        \"teacher_forcing_ratio\": 0.5,\n",
        "        \"clip\": 1.0,\n",
        "        \"epochs\": 30\n",
        "    },\n",
        "    \"model4\": {\n",
        "        \"tokenizer\": \"bpe_8000\",\n",
        "        \"batch_size\": 256,\n",
        "        \"embedding_dim\": 256,\n",
        "        \"hidden_dim\": 512,\n",
        "        \"n_layers\": 4,\n",
        "        \"dropout\": 0.3,\n",
        "        \"teacher_forcing_ratio\": 0.5,\n",
        "        \"clip\": 0.5,\n",
        "        \"epochs\": 25\n",
        "    },\n",
        "    \"model5\": {\n",
        "        \"tokenizer\": \"unigram_8000\",\n",
        "        \"batch_size\": 128,\n",
        "        \"embedding_dim\": 256,\n",
        "        \"hidden_dim\": 512,\n",
        "        \"n_layers\": 2,\n",
        "        \"dropout\": 0.5,\n",
        "        \"teacher_forcing_ratio\": 0.25,\n",
        "        \"clip\": 1.0,\n",
        "        \"epochs\": 30\n",
        "    },\n",
        "    \"model6\": {\n",
        "        \"tokenizer\": \"unigram_8000\",\n",
        "        \"batch_size\": 256,\n",
        "        \"embedding_dim\": 512,\n",
        "        \"hidden_dim\": 512,\n",
        "        \"n_layers\": 4,\n",
        "        \"dropout\": 0.5,\n",
        "        \"teacher_forcing_ratio\": 0.25,\n",
        "        \"clip\": 1.0,\n",
        "        \"epochs\": 25\n",
        "    },\n",
        "    \"model7\": {\n",
        "        \"tokenizer\": \"unigram_8000\",\n",
        "        \"batch_size\": 256,\n",
        "        \"embedding_dim\": 512,\n",
        "        \"hidden_dim\": 512,\n",
        "        \"n_layers\": 4,\n",
        "        \"dropout\": 0.5,\n",
        "        \"teacher_forcing_ratio\": 0.5,\n",
        "        \"clip\": 1.0,\n",
        "        \"epochs\": 30\n",
        "    },\n",
        "    \"model8\": {\n",
        "        \"tokenizer\": \"unigram_16000\",\n",
        "        \"batch_size\": 256,\n",
        "        \"embedding_dim\": 512,\n",
        "        \"hidden_dim\": 512,\n",
        "        \"n_layers\": 4,\n",
        "        \"dropout\": 0.5,\n",
        "        \"teacher_forcing_ratio\": 0.6,\n",
        "        \"clip\": 1.0,\n",
        "        \"epochs\": 30\n",
        "    }\n",
        "}"
      ],
      "metadata": {
        "id": "95Zr9J_HC7IX"
      },
      "id": "95Zr9J_HC7IX",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_file = \"data/gleu/test.txt\"\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# load all models and their parameters\n",
        "for model_name, params in model_parameters.items():\n",
        "    tokenizer_model_filename = f'tokenizer_models/{params[\"tokenizer\"]}.model'\n",
        "    sp = spm.SentencePieceProcessor(model_file=tokenizer_model_filename)\n",
        "\n",
        "    model = Seq2Seq(\n",
        "        Encoder(\n",
        "            input_dim=sp.get_piece_size(),\n",
        "            embedding_dim=params[\"embedding_dim\"],\n",
        "            hidden_dim=params[\"hidden_dim\"],\n",
        "            n_layers=params[\"n_layers\"],\n",
        "            dropout=params[\"dropout\"]\n",
        "        ),\n",
        "        Decoder(\n",
        "            output_dim=sp.get_piece_size(),\n",
        "            embedding_dim=params[\"embedding_dim\"],\n",
        "            hidden_dim=params[\"hidden_dim\"],\n",
        "            n_layers=params[\"n_layers\"],\n",
        "            dropout=params[\"dropout\"]\n",
        "        ),\n",
        "        device\n",
        "    )\n",
        "\n",
        "    # load the model state\n",
        "    model.load_state_dict(torch.load(f\"models/{model_name}.pt\"))\n",
        "\n",
        "    # get special token IDs\n",
        "    sos_token_id = sp.bos_id()\n",
        "    eos_token_id = sp.eos_id()\n",
        "\n",
        "    output_dir = os.path.dirname(output_file)\n",
        "    if not os.path.exists(output_dir):\n",
        "        os.makedirs(output_dir)\n",
        "\n",
        "    # define output file path\n",
        "    output_file = f\"data/gleu/result/test/{model_name}.txt\"\n",
        "\n",
        "    # generate corrections\n",
        "    generate_corrections(test_file, output_file, model, sp, sos_token_id, eos_token_id, device)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dONZaVd3DT91",
        "outputId": "0217ab91-04dc-4374-b878-0a7c66fd7174"
      },
      "id": "dONZaVd3DT91",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-15-9f1d88846534>:28: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load(f\"models/{model_name}.pt\"))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Corrected sentences saved to data/gleu/result/test/model1.txt\n",
            "Corrected sentences saved to data/gleu/result/test/model2.txt\n",
            "Corrected sentences saved to data/gleu/result/test/model3.txt\n",
            "Corrected sentences saved to data/gleu/result/test/model4.txt\n",
            "Corrected sentences saved to data/gleu/result/test/model5.txt\n",
            "Corrected sentences saved to data/gleu/result/test/model6.txt\n",
            "Corrected sentences saved to data/gleu/result/test/model7.txt\n",
            "Corrected sentences saved to data/gleu/result/test/model8.txt\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.6"
    },
    "colab": {
      "provenance": [],
      "gpuType": "L4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}