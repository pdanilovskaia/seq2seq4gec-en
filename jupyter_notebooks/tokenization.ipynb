{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Tokenization\n",
    "\n",
    "This notebook contains tokenization with SentencePiecce, an unsupervised text tokenizer developed by [Google](https://github.com/google/sentencepiece).\n",
    " \n",
    "* Its vocabulary size is predetermined prior to the neural model training ( 8k, 16k, or 32k )\n",
    "* It is used mainly for Neural Network-based text generation systems\n",
    "* It implements subword units using byte-pair-encoding (BPE) and unigram language model \n",
    "* It can be trained directly from raw sentences\n",
    "* It does not depend on language-specific pre/postprocessing\n"
   ],
   "id": "f08c465d1829cf82"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-07-30T16:49:22.272991Z",
     "start_time": "2024-07-30T16:49:22.271078Z"
    }
   },
   "source": "#!pip install sentencepiece",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Firstly, we load the data.",
   "id": "551a9d782bf0bb27"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-30T16:49:22.306209Z",
     "start_time": "2024-07-30T16:49:22.273748Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import json\n",
    "\n",
    "data_dict = {}\n",
    "\n",
    "for dataset_name in ['train', 'dev', 'test']:\n",
    "    filename = f'data/preprocessed_data/{dataset_name}.json'\n",
    "    \n",
    "    with open(filename, 'r') as f:\n",
    "        data_dict[dataset_name] = json.load(f)\n",
    "\n",
    "train_data = data_dict['train']\n",
    "dev_data = data_dict['dev']\n",
    "test_data = data_dict['test']\n",
    "\n",
    "print(f'Train data loaded: {len(train_data)} entries')\n",
    "print(f'Dev data loaded: {len(dev_data)} entries')\n",
    "print(f'Test data loaded: {len(test_data)} entries')\n"
   ],
   "id": "46ec88fe844ffb3f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data loaded: 34000 entries\n",
      "Dev data loaded: 3368 entries\n",
      "Test data loaded: 979 entries\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "In order to train the tokenizer, we need to pass it a txt file with both source and target sentences. So we prepare the data accordingly.",
   "id": "6331dbfa60afe6cd"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-30T16:49:22.319842Z",
     "start_time": "2024-07-30T16:49:22.306696Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Prepare training data\n",
    "import os\n",
    "os.makedirs('data/tokenizer_input', exist_ok=True)\n",
    "with open('data/tokenizer_input/train.txt', 'w') as f:\n",
    "    for entry in train_data:\n",
    "        f.write(entry['src'] + '\\n')\n",
    "        f.write(entry['tgt'] + '\\n')\n",
    "    print(f'Train data saved to data/tokenizer_input/train.txt')\n"
   ],
   "id": "ef851310e797f48c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data saved to data/tokenizer_input/train.txt\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "After the data has been prepared, we can train the tokenizer with  various parameters for vocabulary and model type.",
   "id": "f208caf195cd8b3a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-30T16:51:13.919101Z",
     "start_time": "2024-07-30T16:51:07.193364Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import sentencepiece as spm\n",
    "\n",
    "os.makedirs('tokenizer_models', exist_ok=True)\n",
    "\n",
    "# Configurations\n",
    "vocab_sizes = [8000, 16000]\n",
    "model_types = ['unigram', 'bpe']\n",
    "input_file = 'data/tokenizer_input/train.txt'\n",
    "\n",
    "# Train and save models for each configuration\n",
    "for vocab_size in vocab_sizes:\n",
    "    for model_type in model_types:\n",
    "        model_prefix = f'tokenizer_models/{model_type}_{vocab_size}'\n",
    "        spm.SentencePieceTrainer.train(input=input_file,\n",
    "                                       vocab_size=vocab_size,\n",
    "                                       model_prefix=model_prefix,\n",
    "                                       model_type=model_type,\n",
    "                                       pad_id=0, unk_id=1,\n",
    "                                       bos_id=2, eos_id=3,\n",
    "                                       pad_piece='[PAD]',\n",
    "                                       unk_piece='[UNK]',\n",
    "                                       bos_piece='[BOS]', \n",
    "                                       eos_piece='[EOS]')\n",
    "        print(f'Model trained and saved: {model_prefix}.model')\n",
    "\n"
   ],
   "id": "32469adb87819de6",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model trained and saved: tokenizer_models/unigram_8000.model\n",
      "Model trained and saved: tokenizer_models/bpe_8000.model\n",
      "Model trained and saved: tokenizer_models/unigram_16000.model\n",
      "Model trained and saved: tokenizer_models/bpe_16000.model\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Following methods help add tokenized info to our data (that is representing each sentence as tokens and ids with the latter ones being stored as tensors).",
   "id": "3a8e8bd5a19e7786"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-30T16:51:37.140686Z",
     "start_time": "2024-07-30T16:51:37.137144Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def tokenize_sentences(sentences, sp):\n",
    "    tokenized_data = []\n",
    "    for sentence in sentences:\n",
    "        # Get token IDs including BOS and EOS\n",
    "        ids = [sp.bos_id()] + sp.encode_as_ids(sentence) + [sp.eos_id()]\n",
    "        # Get token pieces including BOS and EOS\n",
    "        tokens = ['BOS'] + sp.encode_as_pieces(sentence) + ['EOS']\n",
    "        tokenized_data.append((ids, tokens))\n",
    "    return tokenized_data"
   ],
   "id": "d607cc00e3b668c2",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-30T16:51:40.220856Z",
     "start_time": "2024-07-30T16:51:39.175801Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch \n",
    "def prepare_tokenized_data(src_sentences, tgt_sentences, sp):\n",
    "    src_data = tokenize_sentences(src_sentences, sp)\n",
    "    tgt_data = tokenize_sentences(tgt_sentences, sp)\n",
    "    tokenized_data = []\n",
    "    for i in range(len(src_sentences)):\n",
    "        src_ids, src_tokens = src_data[i]\n",
    "        tgt_ids, tgt_tokens = tgt_data[i]\n",
    "        tokenized_data.append({\n",
    "            'src': src_sentences[i],\n",
    "            'tgt': tgt_sentences[i],\n",
    "            'src_tokens': src_tokens,\n",
    "            'tgt_tokens': tgt_tokens,\n",
    "            'src_ids': torch.tensor(src_ids, dtype=torch.int64),\n",
    "            'tgt_ids': torch.tensor(tgt_ids, dtype=torch.int64)\n",
    "        })\n",
    "    return tokenized_data"
   ],
   "id": "e4b5b100997adeef",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-30T16:51:40.800323Z",
     "start_time": "2024-07-30T16:51:40.790813Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Extract sentences from the training, dev, and test data\n",
    "src_train = [entry['src'] for entry in train_data]\n",
    "tgt_train = [entry['tgt'] for entry in train_data]\n",
    "\n",
    "src_dev = [entry['src'] for entry in dev_data]\n",
    "tgt_dev = [entry['tgt'] for entry in dev_data]\n",
    "\n",
    "src_test = [entry['src'] for entry in test_data]\n",
    "tgt_test = [entry['tgt'] for entry in test_data]"
   ],
   "id": "a89e3edfebaecdd",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-30T17:08:59.624348Z",
     "start_time": "2024-07-30T17:08:31.407187Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Tokenize the train, dev, and test datasets using each model\n",
    "tokenized_datasets = {}\n",
    "\n",
    "for model_name in ['bpe_8000', 'bpe_16000', 'unigram_8000', 'unigram_16000']:\n",
    "    print(f'Tokenizing with model: {model_name}')\n",
    "    # Load the SentencePiece model\n",
    "    model_filename = 'tokenizer_models/' + model_name + '.model'\n",
    "    sp = spm.SentencePieceProcessor(model_file=model_filename)\n",
    "    train_tokenized = prepare_tokenized_data(src_train, tgt_train, sp)\n",
    "    dev_tokenized = prepare_tokenized_data(src_dev, tgt_dev, sp)\n",
    "    test_tokenized = prepare_tokenized_data(src_test, tgt_test, sp)\n",
    "    \n",
    "    tokenized_datasets[model_name] = {\n",
    "        'train': train_tokenized,\n",
    "        'dev': dev_tokenized,\n",
    "        'test': test_tokenized\n",
    "    }\n",
    "\n",
    "# Save the tokenized datasets to JSON files\n",
    "output_dir = 'data/tokenized_data'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "for model_name, datasets in tokenized_datasets.items():\n",
    "    for dataset_type in ['train', 'dev', 'test']:\n",
    "        filename = f'{output_dir}/{model_name}_{dataset_type}.pt'\n",
    "        torch.save(datasets[dataset_type], filename)\n",
    "        print(f'Successfully saved {dataset_type} data for {model_name} to {filename}')"
   ],
   "id": "9978879eb15c2bd2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing with model: bpe_8000\n",
      "Tokenizing with model: bpe_16000\n",
      "Tokenizing with model: unigram_8000\n",
      "Tokenizing with model: unigram_16000\n",
      "Successfully saved train data for bpe_8000 to data/tokenized_data/bpe_8000_train.pt\n",
      "Successfully saved dev data for bpe_8000 to data/tokenized_data/bpe_8000_dev.pt\n",
      "Successfully saved test data for bpe_8000 to data/tokenized_data/bpe_8000_test.pt\n",
      "Successfully saved train data for bpe_16000 to data/tokenized_data/bpe_16000_train.pt\n",
      "Successfully saved dev data for bpe_16000 to data/tokenized_data/bpe_16000_dev.pt\n",
      "Successfully saved test data for bpe_16000 to data/tokenized_data/bpe_16000_test.pt\n",
      "Successfully saved train data for unigram_8000 to data/tokenized_data/unigram_8000_train.pt\n",
      "Successfully saved dev data for unigram_8000 to data/tokenized_data/unigram_8000_dev.pt\n",
      "Successfully saved test data for unigram_8000 to data/tokenized_data/unigram_8000_test.pt\n",
      "Successfully saved train data for unigram_16000 to data/tokenized_data/unigram_16000_train.pt\n",
      "Successfully saved dev data for unigram_16000 to data/tokenized_data/unigram_16000_dev.pt\n",
      "Successfully saved test data for unigram_16000 to data/tokenized_data/unigram_16000_test.pt\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Experimenting with vocabulary\n",
    "\n",
    "This section shows some examples from tokenized data."
   ],
   "id": "3b77f1e1efaaa686"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-30T17:09:13.769996Z",
     "start_time": "2024-07-30T17:09:11.237830Z"
    }
   },
   "cell_type": "code",
   "source": "train_data = torch.load('data/tokenized_data/bpe_8000_train.pt')",
   "id": "17cb2990bbf5a7e5",
   "outputs": [],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-30T17:09:30.094109Z",
     "start_time": "2024-07-30T17:09:30.091021Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Print tokenized data examples\n",
    "print(\"Tokenized Train Data:\")\n",
    "print(train_data[0])"
   ],
   "id": "42c9f503461afbee",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized Train Data:\n",
      "{'src': 'My town is a medium size city with eighty thousand inhabitants .', 'tgt': 'My town is a medium - sized city with eighty thousand inhabitants .', 'src_tokens': ['BOS', '▁My', '▁town', '▁is', '▁a', '▁medium', '▁size', '▁city', '▁with', '▁eight', 'y', '▁thousand', '▁inhabitants', '▁.', 'EOS'], 'tgt_tokens': ['BOS', '▁My', '▁town', '▁is', '▁a', '▁medium', '▁-', '▁s', 'ized', '▁city', '▁with', '▁eight', 'y', '▁thousand', '▁inhabitants', '▁.', 'EOS'], 'src_ids': tensor([   2,  453,  703,   51,    5, 6149, 3413,  459,  103, 3198, 7946, 4656,\n",
      "        5970,   11,    3]), 'tgt_ids': tensor([   2,  453,  703,   51,    5, 6149,  232,   12, 1282,  459,  103, 3198,\n",
      "        7946, 4656, 5970,   11,    3])}\n"
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-30T17:09:33.018640Z",
     "start_time": "2024-07-30T17:09:33.010708Z"
    }
   },
   "cell_type": "code",
   "source": "sp = spm.SentencePieceProcessor(model_file='tokenizer_models/bpe_8000.model')",
   "id": "946940334e10b026",
   "outputs": [],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-30T17:09:34.034511Z",
     "start_time": "2024-07-30T17:09:34.031328Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#returns vocab size\n",
    "print(sp.get_piece_size())"
   ],
   "id": "a26c5da827456c9f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000\n"
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-30T17:09:35.064167Z",
     "start_time": "2024-07-30T17:09:35.059751Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print('bos=', sp.bos_id())\n",
    "print('eos=', sp.eos_id())\n",
    "print('unk=', sp.unk_id())\n",
    "print('pad=', sp.pad_id())"
   ],
   "id": "a0d967034798c7a0",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bos= 2\n",
      "eos= 3\n",
      "unk= 1\n",
      "pad= 0\n"
     ]
    }
   ],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-30T17:09:35.979451Z",
     "start_time": "2024-07-30T17:09:35.969334Z"
    }
   },
   "cell_type": "code",
   "source": [
    "vocabs = [sp.id_to_piece(id) for id in range(sp.get_piece_size())]\n",
    "vocabs[:10]"
   ],
   "id": "67496dccf3221737",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[PAD]', '[UNK]', '[BOS]', '[EOS]', '▁t', '▁a', 'he', 'in', 're', '▁w']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-30T17:09:37.663022Z",
     "start_time": "2024-07-30T17:09:37.659162Z"
    }
   },
   "cell_type": "code",
   "source": "type(train_data[0][\"src_ids\"])",
   "id": "5779225ea753aea9",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Tensor"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 23
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
